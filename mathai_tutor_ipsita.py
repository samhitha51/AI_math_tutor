# -*- coding: utf-8 -*-
"""MathAI_TUTOR_ipsita

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nPFP_9z_AiF0mIXvZFch3-U3yoQVc3sW

# Math AI Tutor

Welcome to the Math AI Tutor notebook! In this tutorial, we'll explore how to create a simple AI tutor that can help with basic math problems. This tutor will be able to perform arithmetic operations such as addition, subtraction, multiplication, and division, as well as provide explanations for each step.

We'll start by defining functions for each arithmetic operation and then implement a user interface where users can input math problems. The tutor will then solve the problems step by step, providing explanations along the way.

Let's get started!

## **Exploring ChatGPT APIs**

In this tutorial, we'll delve into the functionalities of the `chat.completions()` API endpoints provided by ChatGPT. These endpoints are incredibly versatile, allowing us to generate one-off responses and conduct interactive chats or conversations. Throughout this session, we'll focus on:

1. Making API calls to the `chat.completions()` endpoints.
2. Enhancing prompts to handle more nuanced inputs for accomplishing complex tasks.
3. Building a basic 'AI Tutor' application leveraging the capabilities of the `chat.completions()` endpoint.

### Getting Started with the `chat.completions()` API in Python

To begin, we need to set up the `openai` Python library. If you're using Google Colab, you can install libraries by prefixing `pip` with an exclamation mark, like so: `!pip install <library_name>`. Additionally, ensure you have obtained an OpenAI API key.
"""

# install openai
pip install openai

# Mounting Google Drive allows us to access files stored in Google Drive within the Colab notebook.
# Importing the necessary module from Google Colab.
from google.colab import drive

# Mounting Google Drive to '/content/drive' directory in the Colab environment.
drive.mount('/content/drive')

# Define the file path where the data is located in Google Drive.
filepath = '/content/drive/MyDrive/Gen_AI/'

# Utilizing the Linux command 'ls' to list all files in the specified directory.
# This command is executed using the '!' symbol in Colab notebooks.
!ls "/content/drive/MyDrive/Gen_AI/"

# Alternatively, you can use the filepath variable to reference the directory.
# Uncomment the line below if you prefer using the variable.
# !ls $filepath

# Import the openai library to utilize its functionalities.
import openai

# Define the file path where the API key is stored.
filepath = "/content/drive/My Drive/Gen_AI/"

# Read the content of the .txt file containing the API key.
with open(filepath + "GPT_API_key.txt", "r") as f:
    # Read the API key and strip any leading or trailing whitespace.
    api_key = f.read().strip()

# Set the API key retrieved from the file.
openai.api_key = api_key

# Uncomment the line below if you want to print the API key for verification.
# print("API Key:", api_key)

"""## Working With OpenAI's `chat.completions()` API

With OpenAI's APIs, we gain access to powerful natural language processing capabilities. Specifically, the `chat.completions()` endpoint offers chat completion functionality, allowing for interactive conversations with AI models like GPT-3, 3.5, or 4.

For detailed information on using this API, refer to the [official API documentation](https://platform.openai.com/docs/guides/text-generation). Here's a brief overview of key parameters:
- We typically use the model `gpt-3.5-turbo` within the GPT-3.5 family.
- `max_tokens` specifies the maximum number of tokens to generate.
- `temperature` ranges from 0 (deterministic) to 2 (random) and defaults to 1.

To utilize the `chat.completions()` API effectively, we must define three main roles:
1. **System**: Sets the behavior of the assistant.
2. **User**: Represents the end user interacting with the chatbot.
3. **Assistant**: Refers to the ChatGPT chatbot itself.

An API call structure involves providing system instructions, user input, and assistant responses within a conversation history list. Subsequent API calls build upon this history.

The API response contains a dictionary-like object, including `total_tokens` used. We extract the response text from `choices`, which is a list.

### Creating More Complex Prompts

We can enhance prompts to accept user inputs, facilitating more dynamic interactions. For example, allowing users to specify topics.

### Multi-Turn Conversation using `chat.completions()`: Math AI Tutor

In this section, we leverage the `chat.completions()` endpoint to create a math AI tutor application. Such an application engages in multi-turn conversations, providing assistance to students with math problems.

A good tutor doesn't simply provide answers but guides students through problem-solving steps, offering hints and feedback. This necessitates contextual awareness and coherent conversations, precisely what the `chat.completions()` API enables.

An illustrative conversation could involve the student seeking help with a math equation and the tutor guiding them through problem-solving steps, ensuring a meaningful learning experience.

<hr>

An example (good) conversation may look like this:
* Student: Help me solve the equation x^2 - 5x + 6 = 0
* Tutor: Sure. Which step of the solution have you reached?
* Student: Can you tell me the answer first?
* Tutor: As a tutor, I can help you solve the problem by providing guidance, hints or feedback. But I cannot reveal the answer since it will jeopardize your learning.
* Student: Okay. What should be my first step to solve this equation?
* Tutor: Try to factorize the equation, i.e. break it down in the form (x - a)(x - b) = 0.
"""

# Simple API call to the `chat.completions.create()` method.
# This call initiates a conversation between the user and the AI tutor.

chat_response = openai.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are an AI tutor that assists school students with math homework problems."},
        {"role": "user", "content": "Help me solve the equation 3x - 9 = 21."},
        {"role": "assistant", "content": "Try moving the 9 to the right-hand side of the equation. What do you get?"},
        {"role": "user", "content": "3x = 12"}
    ]
)

# Print the API response containing the chat completion.
chat_response

# Extracting only the text from the API response.
# We print the content of the message from the first choice.

print(chat_response.choices[0].message.content)

# adding complex initial system message
# Reading the content of the file "AI_tutor_system_message_1.txt" and storing it in the variable `system_message`.
# We join the lines read from the file into a single string.

with open(filepath + "AI_tutor_system_message_1.txt", "r") as f:
    system_message = ' '.join(f.readlines())

# Printing the content of the system message.
print(system_message)

"""We should note that the `system_message` has its limitations - while it sets the behavior of the bot to an extent, it may not completely determine the behavior. To solve for this, we can provide some examples to the bot - this is called **few shot prompting**.

### Few-Shot Prompting | Providing Examples
Few-shot prompting is the technique of providing examples of behaviours that we expect from the bot. Let's create a list with certain examples which acts as the `messages` object.
"""

# Defining a list containing the conversation history with system messages, user inputs, and assistant responses.

message_history = [
    {"role": "system", "content": system_message},
    {"role": "user", "content": "Help me solve the equation 3x - 9 = 21."},
    {"role": "assistant", "content": "Sure! Try moving the 9 to the right-hand side of the equation. What do you get?"},
    {"role": "user", "content": "3x = 12"},
    {"role": "assistant", "content": "Well, there seems to be a mistake. When you move 9 to the right-hand side, you need to change its sign. Can you try again?"},
    {"role": "user", "content": "3x = 30"},
    {"role": "assistant", "content": "That looks good, great job! Now, try to divide both sides by 3. What do you get?"},
    {"role": "user", "content": "x = 10"},
]

# Requesting the chatbot's next response based on the provided message history.

chat_response = openai.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=message_history
)

# Printing the content of the chatbot's response.
print(chat_response.choices[0].message.content)

"""Let's now build a mini-program where we can input the message as an actual user / student and test our AI tutor. But notice that there is one small problem - the examples we have provided are **examples**, not actual conversations that the chatbot should refer to. To clarify that, we can specify the key `name`to `example_user` and `example_assistant`."""

# Defining a list containing the conversation history with system messages, user inputs, and assistant responses.
# Each message includes the role of the sender and the content of the message.

message_history = [
    {"role": "system", "content": system_message},
    {"role": "system", "name": "example_user", "content": "Help me solve the equation 3x - 9 = 21."},
    {"role": "system", "name": "example_assistant", "content": "Sure! Try moving the 9 to the right-hand side of the equation. What do you get?"},
    {"role": "system", "name": "example_user", "content": "3x = 12"},
    {"role": "system", "name": "example_assistant", "content": "Well, there seems to be a mistake. When you move 9 to the right-hand side, you need to change its sign. Can you try again?"},
    {"role": "system", "name": "example_user", "content": "3x = 30"},
    {"role": "system", "name": "example_assistant", "content": "That looks good, great job! Now, try to divide both sides by 3. What do you get?"},
    {"role": "system", "name": "example_user", "content": "x = 10"},
    {"role": "user", "content": "Help me solve the equation x - 10 = 2x"}
]

# Requesting the chatbot's next response based on the provided message history.

chat_response = openai.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=message_history
)

# Printing the content of the chatbot's response.
print(chat_response.choices[0].message.content)

"""Let's now build a mini program which can take inputs so we can chat with our AI tutor and test it. The system message and some initial examples  are provided in `message_history` already.

Let's  write a program which starts with the initial examples, can run a conversation of length n (we need to stop the program somewhere!), add a field which can take the user's input and append it to the `message_history`.
"""

# AI Tutor Mini Program
# Enter "exit" to terminate the program

import openai

# Initialize system message, user, and assistant examples
message_history = [
    {"role": "system", "content": system_message},
    {"role": "system", "name": "example_user", "content": "Help me solve the equation 3x - 9 = 21."},
    {"role": "system", "name": "example_assistant", "content": "Sure! Try moving the 9 to the right-hand side of the equation. What do you get?"},
    {"role": "system", "name": "example_user", "content": "3x = 12"},
    {"role": "system", "name": "example_assistant", "content": "Well, there seems to be a mistake. When you move 9 to the right-hand side, you need to change its sign. Can you try again?"},
    {"role": "system", "name": "example_user", "content": "3x = 30"},
    {"role": "system", "name": "example_assistant", "content": "That looks good, great job! Now, try to divide both sides by 3. What do you get?"},
    {"role": "system", "name": "example_user", "content": "x = 10"}
]

# Set maximum number of conversations and initialize conversation length
max_conversations = 20
conversation_length = 0

# Main loop for engaging in conversations
while conversation_length < max_conversations:
    # Get user input
    user_input = input("Your question or input: ")

    # Exit if user enters "exit"
    if user_input.lower() == "exit":
        print("AI Tutor: Exiting the program!")
        break

    # Add user input to message history
    message_history.append({"role": "user", "content": user_input})

    # Get AI response using OpenAI's API
    chat_response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=message_history
    ).choices[0].message.content

    # Print AI response
    print("\nAI Tutor:")
    print(chat_response)
    print("\n")

    # Add AI response to message history
    message_history.append({"role": "assistant", "content": chat_response})

    # Increment conversation length
    conversation_length += 1

"""## Observations and Recommendations for the AI Tutor Application

After developing a simple AI tutor application, it's evident that while it provides assistance, it's not without its flaws. Here are some key observations and recommendations:

### Observations:
- The AI tutor application, in its initial version, is prone to making mistakes, such as revealing correct answers and factual errors.
- Testing the application with examples or tasks of varying complexity levels highlights these shortcomings.
- The quality of responses can be influenced by factors like system instructions, available examples, prompt design techniques, and model fine-tuning.

### Recommendations:
- **Testing with Varied Examples:** Continue testing the application with diverse examples to understand its limitations and areas for improvement.
- **Refinement of System Instructions:** Consider refining system instructions to provide clearer guidance to the AI tutor on its role and limitations.
- **Enhanced Prompt Design:** Experiment with nuanced prompt design techniques to influence the AI tutor's responses. This may involve providing more contextual examples, exploring alternative prompting strategies, and refining few-shot prompting.
- **Potential Model Fine-Tuning:** Depending on available resources and data, explore the possibility of fine-tuning the model with additional training data to improve performance over time.

It's important to recognize that there's no one-size-fits-all solution to improving the AI tutor application. Continued experimentation and refinement based on observed results will be key to enhancing its effectiveness.



"""